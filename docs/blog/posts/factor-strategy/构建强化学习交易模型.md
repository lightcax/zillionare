!!! Abstract
    强化学习已在摩根大通及全球顶级投资机构中使用。
    与监督学习不同，强化学习不会在每一步都只接受标准答案，它会尝试、忍受短期的损失，博取长期的收益。这就使得它有了对抗金融数据噪声的能力。
    奖励是强化学习的灵魂。我们可以直接把投资组合的收益率作为奖励。在监督学习中，损失函数是其核心，但我们却无法把收益率作为损失函数。
    本文附有完整、精彩的强化学习代码，且不依赖于FinRL等框架，可在国内市场运行。

强化学习（RL）这个名字，第一次闯入大众视野，还要追溯到AlphaGo与李世石那场载入史册的人机大战。一战成名后，它似乎又回归了学术的象牙塔，直到最近，随着DeepSeek等模型的惊艳亮相，RL以其强大的推理能力，再次被推到了聚光灯下。

其实，强化学习在量化投资中早有实际的应用。尽管一些顶尖的投资公司的当家策略不会轻易透露出来，我们还是找到了一些案例，表明华尔街的顶级玩家们早已开始使用强化学习。

比如，2017年前后，全球顶级的投资银行摩根大通（J.P. Morgan）就推出了一个名为LOXM<sup>1</sup>的“觅影”交易执行平台。而驱动这个平台的『秘密武器』，正是我们今天的主角——强化学习（Reinforcement Learning, RL）。

LOXM的目标非常明确：在执行大额股票订单时，像一个顶级交易员一样，智能地将大单拆分成无数小单，在复杂的市场微观结构中穿梭，以最低的冲击成本和最快的速度完成交易。这已经不是简单地预测涨跌，而是在动态的市场博弈中，学习“如何交易”这门艺术。

## 究竟什么是强化学习？
那么，这个听起来如此高大上的强化学习，到底是什么？

根据《Reinforcement Learning for Quantitative Trading》这篇文章，我们可以构建一个统一的框架来理解它。

想象一下，你正在玩一个电子游戏，你的目标是获得尽可能高的分数。在这个游戏里：

- 你，就是代理（Agent）。在量化交易中，这个代理就是你的交易算法。
- 游戏世界，就是环境（Environment）。在交易中，这就是瞬息万变的金融市场。
- 你在游戏中看到的画面和状态（比如你的血量、位置、敌人的数量），就是状态（State）。在交易中，这可以是股价、成交量、技术指标、宏观数据等等。
- 你按下的每一个操作（前进、后退、开火），就是行动（Action） 。在交易中，这对应着买入、卖出或持有。
- 你每次行动后获得或失去的分数 ，就是奖励（Reward）。在交易中，这通常是你的投资组合的收益或损失。
强化学习的核心思想，就是让代理（交易算法）在这个环境（金融市场）中不断地“试错”（take actions），根据每次试错后得到的奖励（收益或亏损），来学习一套最优的策略（Policy），从而在长期内实现累计奖励的最大化（长期收益最大化）。它不是在学习“市场下一秒会怎样”，而是在学习**『面对当前的市场，我该怎么做才是最优的』**。

## 强化学习强在哪儿？

看到这里，你可能会问，我们已经有了监督学习（比如预测股价涨跌）和无监督学习（比如聚类发现市场风格），为什么还需要强化学习？它到底强在哪？

强化学习与与监督/无监督学习的根本区别在于学习范式。监督学习像是在背一本标准答案书。你给它一张历史K线图（输入特征），告诉它第二天是涨还是跌（标签），它学习的是一种静态的"看图识字"能力。无监督学习则是在没有答案的情况下，自己在一堆数据里找规律，比如把相似的股票自动归为一类。它们都在试图回答"是什么"的问题。

而强化学习，则是在学习一套决策流程。它没有"标准答案"可背。市场不会告诉你"在此时此刻买入就是唯一正确的答案"。RL面对的是一系列的决策，每个决策都会影响到未来的状态和可能的收益。它要回答的是"该做什么"的问题。这是一个动态的、有因果链条的、面向未来的学习过程。

有人会说，我可以用监督学习模型，然后不断地用新的数据去持续训练和预测（即在线学习，live learning），这和强化学习有什么区别？

表面上看，两者都在不断适应新数据，但内核完全不同。而强化学习的核心优势在于两个监督学习无法企及的维度：

!!! tip 探索与利用（Exploration vs. Exploitation）
    这是RL的灵魂。想象一下，你常去的一家餐厅味道不错（利用），但你偶尔也会想试试新馆子，万一有惊喜呢（探索）？RL代理在训练时，也会在"执行已知的最优策略"和"尝试未知的、可能有更高回报的策略"之间进行权衡。这种探索精神，使得RL有可能发现人类交易员或监督学习模型从未想过的、更优的交易模式。而监督学习只会告诉你，根据历史经验，去那家老餐厅是"正确答案"。
  
  
!!! tip 学习动态因果关系
    强化学习关注的是一个行动序列（比如，先买入A，再卖出B）与最终结果之间的因果联系。它能理解，有时候一个短期的亏损（比如为了建仓拉低成本而主动承受浮亏）是为了一个更大的长期目标。而监督学习一次只看一个"时间切片"，它很难理解这种跨时间的、具有延迟效应的因果逻辑。

由于强化学习的这两个特点，所以，比起监督学习，它能更好地对抗金融噪声 -- 众所周知，深度学习在金融投资领域折戟沉沙，主要就是因为金融数据噪声太大的原因。

金融数据以低信噪比著称，充满了随机波动和"假信号"。强化学习之所以在这样的环境中更具优势，主要源于其独特的设计：

- 关注长期回报，容忍短期阵痛：监督学习模型追求的是在每个时间点上预测的准确率。如果市场噪音让它做出了一个错误的预测，它就会被"惩罚"。而RL的目标是最大化整个交易过程的累计回报。这意味着，它可以有意识地执行一个短期看起来会亏钱的动作（比如，在一个看似要下跌的时刻买入），如果这个动作是其长期制胜策略的一部分（比如，它判断这是一个主力洗盘的假摔）。这种"延迟满足"的特性，让它对短期市场噪音有更强的免疫力。

- 动态适应，而非刻舟求剑：市场风格是会切换的，昨天有效的因子，今天可能就失效了。监督学习模型一旦训练好，其模式就相对固定，像一个刻舟求剑的傻瓜。而RL代理的策略本身就是状态的函数，它被设计为根据环境的变化而动态调整自己的行为。当市场从牛市转向熊市，RL代理能够通过与环境的持续交互，感知到这种变化，并相应地调整其交易策略，从激进做多转为保守甚至做空。这种与生俱来的适应性，是其对抗非平稳市场的关键。

```python
from pathlib import Path
data_home = Path("~/workspace/data").expanduser()
```

## 从"追涨杀跌"的AlphaStock说起

顶级机构的量化策略都是秘而不宣的，即使是摩根大通的LOXM这种公开出圈的模型，其构建与运行机制对普通人来讲，也仍然是无法接触的。那么，作为量化交易者，我们要如何构建自己的强化学习交易模型呢？

2019年，来自清华大学和微软亚洲研究院的团队开发了一个名为 AlphaStock<sup>1</sup> 的模型。这个模型巧妙地将强化学习与注意力机制（Attention Mechanism，没错，就是Transformer模型的核心）相结合，专门用来优化一个古老而有效的策略——追涨杀跌（即动量交易）。

传统的动量策略很简单：买入过去表现好的股票，卖出过去表现差的。但问题是，动量什么时候会持续？什么时候会反转？AlphaStock的聪明之处在于，它不依赖于固定的规则，而是让RL代理去学习。代理观察市场上数百只股票的价格和成交量数据（状态），然后决定在哪些股票上分配多少资金（行动）。如果这个决策在未来一段时间带来了正收益，它就获得正奖励，反之亦然。通过海量历史数据的回测训练，AlphaStock最终就能会了如何动态地识别和利用市场中的动量效应，甚至能在一定程度上规避动量反转的风险。这就像一个武林高手，通过无数次实战，最终练就了对战局的敏锐直觉。

## Get Hands Dirty!动手练一下！

理论听起来很美，但如何付诸实践？我们自己能否做出来一个有用的强化学习模型？接下来，我就演示如何做出一个

对于那些可以『无限』（即不受限）访问yfinance及alpaca_trade_api的同学，我强烈推荐从 FinRL 这个开源库开始。它被誉为"金融领域的OpenAI Gym"，极大地降低了入门门槛。

### 安装 FinRL

首先，确保你有一个Python环境（推荐使用Anaconda或venv创建虚拟环境），然后通过pip安装FinRL及其依赖。

```bash
pip install finrl
```

但是，我们的读者可能多数无法使用yfinance及alpaca_trade_api库。这样，你可能就不得不使用Gymnasium库，自己做多一点工作。

!!! tip
    Gymnasium 是 OpenAI Gym 的继任者，是用于开发和比较强化学习算法的开源工具包。它提供了标准化的环境接口，让研究者和开发者能够更方便地测试算法性能。

### 安装 Gymnasium和SB3！

如果不用FinRL，我们就要安装两个名字奇奇怪怪的强化学习库。

!!! tip
    “Gymnasium” 一词源于古希腊语 “γυμνάσιον”（gymnasion），字面意思是 “裸体锻炼的地方”。现代英语中，常指健身房。但在德语中，也指初级中学。

SB3即是Stable-Baselines3，是强化学习领域的主流库之一，凭借其高效性、易用性和丰富的算法支持，成为学术研究和工业应用的首选工具。

这两个库中，Gym是接口，而SB3提供了各种算法。在我们的示例中，将使用它提供的PPO算法。下面是安装这两个库的指南。

```bash
pip install gymnasium
pip install stable_baselines3
```

接下来，需要获取数据。这部分没有啥营养，我们就不放代码出来了，以免占用太多你的阅读时间。你可以使用任何喜欢的数据源，最终需要得到一个以 data和asset为双重索引的DataFrame，并且列名至少包含：open,high,low,close,volume。

!!! tip
    如果你不想写代码，可以报名我们的课程《因子挖掘与机器学习策略》，获取可运行的示例。本示例可以在本地运行。


<!--PAID CONTENT START-->
```python
import pandas as pd
import numpy as np
import talib
import datetime
import warnings
warnings.filterwarnings('ignore')

# 参数设置
N_STOCKS = 50  # 股票数量，可以调整进行多次随机测试
DATA_START_DATE = datetime.date(2010, 1, 1)
DATA_END_DATE = datetime.date(2021, 10, 31)

# 数据划分比例 (train:test)
TRAIN_RATIO = 0.8
```
<!--PAID CONTENT END-->

<!--PAID CONTENT START-->
在示例中，我们使用了缓存数据。如果你要在本地运行这个示例，你可以通过tushare来获取数据，以下为示例：
```python
def get_stock_data_tushare(asset_list, start_date, end_date):
    """使用tushare获取股票数据（兼容方法，返回与load_bars相同格式）"""
    all_data = []

    # 转换日期格式为字符串
    start_str = start_date.strftime("%Y%m%d")
    end_str = end_date.strftime("%Y%m%d")

    for asset in asset_list:
        try:
            # 获取日线数据
            df_stock = pro.daily(ts_code=asset,
                                start_date=start_str,
                                end_date=end_str)

            if not df_stock.empty:
                # 重命名列以匹配load_bars格式
                df_stock = df_stock.rename(columns={
                    'trade_date': 'date',
                    'vol': 'volume'
                })

                # 转换日期格式
                df_stock['date'] = pd.to_datetime(df_stock['date'])

                # 添加asset列
                df_stock['asset'] = asset

                # 按日期排序（tushare返回的数据是倒序的）
                df_stock = df_stock.sort_values('date').reset_index(drop=True)

                # 选择需要的列
                df_stock = df_stock[['date', 'asset', 'open', 'high', 'low', 'close', 'volume']]

                all_data.append(df_stock)
                print(f"成功获取 {asset} 的数据，共 {len(df_stock)} 条记录")
            else:
                print(f"警告：{asset} 没有数据")

        except Exception as e:
            print(f"获取 {asset} 数据时出错: {e}")

    if all_data:
        # 合并所有股票数据
        df_combined = pd.concat(all_data, ignore_index=True)

        # 设置双重索引以匹配load_bars格式
        df_combined = df_combined.set_index(['date', 'asset']).sort_index()

        return df_combined
    else:
        return pd.DataFrame()
```
<!--PAID CONTENT END-->


在量化交易中，我们从来没有见过端到端的人工智能模型能够成功的。基本上，我们总是从特征工程开始，然后才构建机器学习模型。因此，接下来，我们要创建一个FeatureEngineer类，用于处理特征工程。
```python
class FeatureEngineer:
    def __init__(self, use_technical_indicator=True, tech_indicator_list=None):
        self.use_technical_indicator = use_technical_indicator
        self.tech_indicator_list = tech_indicator_list or [
            'macd', 'rsi', 'sma', 'bbands'
        ]

    def preprocess_data(self, df):
        df_reset = df.reset_index()

        processed_stocks = []

        for asset in df_reset["asset"].unique():
            stock_data = df_reset[df_reset["asset"] == asset].copy().sort_values('date')

            # 前向填充价格数据，处理停牌等情况
            price_columns = ['open', 'high', 'low', 'close']
            for col in price_columns:
                if col in stock_data.columns:
                    stock_data[col] = stock_data[col].fillna(method='ffill')

            # 成交量用0填充（停牌时成交量为0是合理的）
            if 'volume' in stock_data.columns:
                stock_data['volume'] = stock_data['volume'].fillna(0)

            close = stock_data['close'].values.astype(float)

            if 'macd' in self.tech_indicator_list:
                macd, macd_signal, macd_hist = talib.MACD(close)
                stock_data['macd'] = macd
                stock_data['macd_signal'] = macd_signal
                stock_data['macd_hist'] = macd_hist

            if 'rsi' in self.tech_indicator_list:
                stock_data['rsi_14'] = talib.RSI(close, timeperiod=14)

            if 'sma' in self.tech_indicator_list:
                stock_data['close_20_sma'] = talib.SMA(close, timeperiod=20)

            if 'bbands' in self.tech_indicator_list:
                bb_upper, bb_middle, bb_lower = talib.BBANDS(close, timeperiod=20)
                stock_data['boll_ub'] = bb_upper
                stock_data['boll_lb'] = bb_lower
                stock_data['boll_middle'] = bb_middle

            # 添加基础指标
            stock_data['returns'] = stock_data['close'].pct_change()
            stock_data['log_volume'] = np.log(stock_data['volume'] + 1)

            processed_stocks.append(stock_data)

        df_processed = pd.concat(processed_stocks, ignore_index=True)

        # 删除包含NaN的行
        before_count = len(df_processed)
        df_processed = df_processed.dropna().reset_index(drop=True)
        after_count = len(df_processed)

        print(f"✅ 技术指标计算完成: {before_count} -> {after_count} 条记录")
        print(f"✅ 价格数据已进行前向填充处理")

        # 显示新增的列
        original_cols = df.reset_index().columns
        new_columns = [col for col in df_processed.columns if col not in original_cols]
        print(f"新增指标: {new_columns}")

        df_processed = df_processed.set_index(['date', "asset"]).sort_index()

        return df_processed
```

为了进行训练，我们需要对数据集进行train/test划分。在PPO强化学习中，我们通过比例来划分数据，确保时间序列的连续性：

```python
def split_data_by_ratio(df, train_ratio=0.8):
    """
    按比例划分数据集为训练集和测试集

    Args:
        df: 输入的DataFrame，必须有date和asset的双重索引
        train_ratio: 训练集比例
        test_ratio: 测试集比例

    Returns:
        tuple: (train_data, test_data)
    """
    train, test = [], []
    def cut_time_series(group, cut):
        itrain = int(len(group) * cut)
        return (group.iloc[:itrain], group.iloc[itrain:])

    for item in df.groupby("asset").apply(cut_time_series, cut=train_ratio).values:
        train.append(item[0])
        test.append(item[1])

    df_train = pd.concat(train)
    df_test = pd.concat(test)

    return df_train, df_test


# 创建特征工程器
fe = FeatureEngineer(
    use_technical_indicator=True,
    tech_indicator_list=['macd', 'rsi', 'sma', 'bbands']
)

# 获取数据并处理特征
raw_data = load_bars(DATA_START_DATE, DATA_END_DATE, N_STOCKS)
processed_data = fe.preprocess_data(raw_data)

# 验证处理后的数据
if processed_data.empty:
    raise ValueError("数据处理后为空，请检查技术指标计算")

# 按比例划分数据集
train_data, test_data = split_data_by_ratio(
    processed_data,
    train_ratio=TRAIN_RATIO
)

# 显示处理后的数据样例
print("\n处理后的数据预览:")
print(train_data.head())
```

!!! tip
    在监督学习中，我们一般会将数据集划分为 train, validation和test 三个部分。其中， train和validation 用于训练模型，在训练完成之后，我们使用test数据集来评估模型性能。这样可以确保训练过程中，完全看不到测试数据，避免过拟合。在强化学习中，我们也同样要划分数据集，但是，根据算法的不同，有可能只需要划分出train和test两个部分。在这里我们使用的PPO算法就只需要划分train和test两个部分。


### 定义环境

通常来说，我们要定义环境和Agent，但是，在使用SB3之后，我们可以直接使用PPO模型，从而无须定义Agent，因为PPO本身就是Agent，所以，Agent的定义和模型训练是一体的。

所以，我们先用gym来定义环境：

```python
import gymnasium as gym
from gymnasium import spaces

class StockTradingEnv(gym.Env):
    """
    基于Gymnasium的股票交易环境

    动作空间: 连续动作，每只股票的买卖比例 [-1, 1]
    状态空间: [现金比例, 持仓比例..., 技术指标...]
    """

    def __init__(self, data, initial_amount=100000, transaction_cost=0.001):
        super().__init__()

        self.data = data.copy()
        self.initial_amount = initial_amount
        self.transaction_cost = transaction_cost

        # 获取股票列表和日期
        self.stock_list = sorted(data.index.get_level_values('asset').unique())
        self.dates = sorted(data.index.get_level_values('date').unique())
        self.data_reset = data.reset_index()

        self.stock_dim = len(self.stock_list)

        # 技术指标列表
        self.tech_indicators = ['macd', 'rsi_14', 'close_20_sma', 'boll_ub', 'boll_lb']

        # 定义动作和观察空间
        # 动作: 每只股票的买卖比例 [-1, 1]
        self.action_space = spaces.Box(
            low=-1, high=1, shape=(self.stock_dim,), dtype=np.float32
        )

        # 观察空间: [现金比例] + [持仓比例...] + [技术指标...]
        obs_dim = 1 + self.stock_dim + len(self.tech_indicators) * self.stock_dim
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32
        )

        # 预先创建价格缓存，避免重复查询
        self._create_price_cache()

        print(f"🏗️  环境初始化完成:")
        print(f"   股票数量: {self.stock_dim}")
        print(f"   交易日数: {len(self.dates)}")
        print(f"   动作维度: {self.action_space.shape}")
        print(f"   状态维度: {self.observation_space.shape}")

        # 初始化状态变量
        self.day = 0
        self.cash = self.initial_amount
        self.holdings = np.zeros(self.stock_dim)
        self.portfolio_value = self.initial_amount
        self.portfolio_history = [self.initial_amount]

    def _create_price_cache(self):
        """创建价格缓存，提高查询效率并处理缺失数据"""
        self.price_cache = {}
        self.tech_cache = {}

        for date in self.dates:
            date_data = self.data_reset[self.data_reset["date"] == date]

            # 价格缓存
            prices = []
            tech_data = []

            for stock in self.stock_list:
                stock_data = date_data[date_data["asset"] == stock]

                if not stock_data.empty and 'close' in stock_data.columns:
                    price = stock_data['close'].iloc[0]
                    prices.append(price)

                    # 技术指标缓存
                    stock_tech = []
                    for indicator in self.tech_indicators:
                        if indicator in stock_data.columns:
                            value = stock_data[indicator].iloc[0]
                            if not np.isnan(value):
                                stock_tech.append(value)
                            else:
                                stock_tech.append(0.0)
                        else:
                            stock_tech.append(0.0)
                    tech_data.extend(stock_tech)
                else:
                    # 如果没有数据，使用前一天的价格（如果有的话）
                    if date in self.price_cache and len(self.price_cache) > 0:
                        # 使用前一个交易日的价格
                        prev_date = max([d for d in self.price_cache.keys() if d < date], default=None)
                        if prev_date:
                            prev_idx = self.stock_list.index(stock)
                            prices.append(self.price_cache[prev_date][prev_idx])
                        else:
                            prices.append(100.0)  # 默认价格
                    else:
                        prices.append(100.0)  # 默认价格

                    # 技术指标用0填充
                    tech_data.extend([0.0] * len(self.tech_indicators))

            self.price_cache[date] = np.array(prices)
            self.tech_cache[date] = np.array(tech_data)
    
    def reset(self, seed=None, options=None):
        super().reset(seed=seed)

        self.day = 0
        self.cash = self.initial_amount
        self.holdings = np.zeros(self.stock_dim)
        self.portfolio_value = self.initial_amount
        self.portfolio_history = [self.initial_amount]

        observation = self._get_observation()
        info = {}

        return observation, info
    
    def step(self, actions):
        # 检查是否结束
        if self.day >= len(self.dates) - 1:
            return self._get_observation(), 0, True, True, {}

        # 获取当前价格（使用缓存）
        current_date = self.dates[self.day]
        prices = self.price_cache.get(current_date, np.zeros(self.stock_dim))

        # 执行交易
        self._execute_trades(actions, prices)

        # 移动到下一天
        self.day += 1

        # 计算新的投资组合价值
        if self.day < len(self.dates):
            next_date = self.dates[self.day]
            next_prices = self.price_cache.get(next_date, prices)
            new_portfolio_value = self.cash + np.sum(self.holdings * next_prices)
        else:
            new_portfolio_value = self.cash + np.sum(self.holdings * prices)

        # 计算奖励
        reward = (new_portfolio_value - self.portfolio_value) / self.portfolio_value if self.portfolio_value > 0 else 0
        self.portfolio_value = new_portfolio_value
        self.portfolio_history.append(self.portfolio_value)

        # 检查是否结束
        terminated = self.day >= len(self.dates) - 1
        truncated = False

        info = {
            'portfolio_value': self.portfolio_value,
            'cash': self.cash,
            'holdings': self.holdings.copy()
        }

        return self._get_observation(), reward, terminated, truncated, info
    
    def _execute_trades(self, actions, prices):
        """
        执行交易动作
        """
        # 计算目标持仓价值
        total_value = self.cash + np.sum(self.holdings * prices)

        for i, action in enumerate(actions):
            if abs(action) < 0.01:  # 忽略很小的动作
                continue

            if prices[i] <= 0:  # 跳过价格为0或负数的股票
                continue

            current_value = self.holdings[i] * prices[i]
            target_value = total_value * max(0, action)  # 只允许正持仓
            trade_value = target_value - current_value

            if trade_value > 0:  # 买入
                cost = trade_value * (1 + self.transaction_cost)
                if cost <= self.cash:
                    shares_to_buy = trade_value / prices[i]
                    self.holdings[i] += shares_to_buy
                    self.cash -= cost
            elif trade_value < 0:  # 卖出
                shares_to_sell = abs(trade_value) / prices[i]
                if shares_to_sell <= self.holdings[i]:
                    self.holdings[i] -= shares_to_sell
                    self.cash += abs(trade_value) * (1 - self.transaction_cost)
    
    def _get_observation(self):
        """
        获取当前状态观察（使用缓存数据，避免递归）
        """
        # 确保day在有效范围内
        current_day = min(self.day, len(self.dates) - 1)
        current_date = self.dates[current_day]

        # 现金比例
        cash_ratio = self.cash / self.portfolio_value if self.portfolio_value > 0 else 0

        # 获取当前价格（使用缓存）
        prices = self.price_cache.get(current_date, np.ones(self.stock_dim) * 100.0)

        # 持仓比例
        holdings_value = self.holdings * prices
        holdings_ratio = holdings_value / self.portfolio_value if self.portfolio_value > 0 else np.zeros_like(holdings_value)

        # 技术指标（使用缓存）
        tech_values = self.tech_cache.get(current_date, np.zeros(len(self.tech_indicators) * self.stock_dim))

        # 标准化技术指标
        normalized_tech = []
        for i, stock in enumerate(self.stock_list):
            for j, indicator in enumerate(self.tech_indicators):
                idx = i * len(self.tech_indicators) + j
                if idx < len(tech_values):
                    value = tech_values[idx]
                    # 标准化技术指标
                    if indicator == 'rsi_14':
                        value = (value - 50) / 50  # RSI标准化到[-1, 1]
                    elif 'macd' in indicator:
                        value = np.tanh(value / 100)  # MACD使用tanh标准化
                    else:
                        close_price = prices[i] if prices[i] > 0 else 1
                        value = np.tanh(value / close_price)
                    normalized_tech.append(value if not np.isnan(value) else 0)
                else:
                    normalized_tech.append(0)

        # 组合观察向量
        observation = np.concatenate([
            [cash_ratio],
            holdings_ratio,
            normalized_tech
        ])

        return observation.astype(np.float32)

# 创建环境前先验证数据
print("🔍 验证数据结构...")
print(f"训练数据形状: {train_data.shape}")
print(f"测试数据形状: {test_data.shape}")

print(f"训练数据 - 股票数量: {len(train_data.index.get_level_values('asset').unique())}")
print(f"训练数据 - 日期数量: {len(train_data.index.get_level_values('date').unique())}")
print(f"测试数据 - 股票数量: {len(test_data.index.get_level_values('asset').unique())}")
print(f"测试数据 - 日期数量: {len(test_data.index.get_level_values('date').unique())}")

# 创建环境
print("\n🏗️  创建交易环境...")

train_env = StockTradingEnv(train_data, initial_amount=100000)
print("✅ 训练环境创建成功")

test_env = StockTradingEnv(test_data, initial_amount=100000)
print("✅ 测试环境创建成功")

print("\n✅ 环境创建完成！")

# 测试环境重置
print("\n🔄 测试环境重置...")
train_obs, _ = train_env.reset()
print(f"训练环境观察向量形状: {train_obs.shape}")

test_obs, _ = test_env.reset()
print(f"测试环境观察向量形状: {test_obs.shape}")
```

首先，我们定义了一个名为StockTradingEnv的类，继承自gym.Env。在强化学习中， Env （环境）就是智能体交互和学习的“世界”。

动作空间是智能体可以执行的操作。这里是连续的，对于我们投资组合里的每一只股票，智能体都可以决定一个介于-1到1之间的值，代表是卖出（-1到0）还是买入（0到1）这只股票的资金比例。

!!! info
    完整地解读这段代码需要很长的篇幅。感兴趣者可以报名《因子挖掘与机器学习策略》课程，获得讲解。

状态空间是智能体观察到的环境信息。它包括了当前的现金比例、每只股票的持仓比例，以及一系列技术指标（比如MACD, RSI等）。这是智能体做决策的依据。在代码中，通过_get_observation方法来构建和获取，返回的信息包含了现金比例，持仓市值、技术指标等。在智能体的每一次决策时，它都会收到这样一个长长的一维数组。

每日持仓记录在self.holdings中，每日资产记录在self.portfolio_history中，用于后续的性能评估和可视化。

reset方法的作用是恢复环境的状态。当一轮完整的交易周期（从头到尾回测）结束后，或者我们想开始新的一轮训练时，就要调用reset方法。

这部分代码中，最核心的方法是step方法。智能体（到目前为止，我们还没有定义智能体，但你马上会看到！）每次执行一个动作 actions ，环境就会调用 step 方法来处理这个动作，并返回结果。

!!! tip
    量化人很熟悉传统回测框架（比如zipline, backtrader）。你可以把step类比成为这些框架中的handle_data或者handle_bar方法。两者都要『执行交易、更新状态』（市值、现金流、收益/奖励）。不过在handle_data中，我们一般要做出决策，而在强化学习的step中，决策部分已经被分离出去了--它被交给了Agent。

在这个框架里，执行交易也变得简单。因为在step方法中，传入的actions已经包含了目标仓位信息，所以，我们只需要根据现有持仓和目标仓位的数量进行计算，就可以知道如何调仓。

<!--
__init__ 方法：初始化我们的世界

这个方法在创建环境实例时被调用，负责设置好我们这个“交易世界”的一切初始参数。

这段代码定义了动作和观察空间

```python
self.action_space = spaces.Box(
            low=-1, high=1, shape=(self.stock_dim,), dtype=np.float32
        )

        obs_dim = 1 + self.stock_dim + len(self.tech_indicators) * self.stock_dim
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32
        )
```

-->

下面，我们就来定义智能体--Agent。
们就来定义智能体--Agent。
## 定义Agent及训练

```python
from stable_baselines3 import PPO

model = PPO(
    "MlpPolicy",
    train_env,
    verbose=1,
    learning_rate=0.0003,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    ent_coef=0.01,
    vf_coef=0.5,
    max_grad_norm=0.5,
    seed=42
)

print("🚀 开始训练...")
model.learn(total_timesteps=10000, progress_bar=True)
print("✅ 训练完成！")

# 保存模型
# model.save("ppo_trading_model")
```

这里我们定义了一个PPO类型的智能体，并且使用了多层感知机（Multi-Layer Perceptron）作为网络结构。对于基于数值向量（现金比例、持股比例、技术指标）的输入，MlpPolicy是最直接和常见的选择。

接下来我们按要求传入环境（这里是train_env），时间步(n_steps)、训练轮数（n_epochs）。这里的时间步是强化学习中的一个核心问题，在后面还有一个total_timesteps参数，我们结合到一起来讲解。

想象一下我们的智能体是一个正在学习交易的学生。他不是每做一笔交易（一个 step ）就马上总结经验、调整策略，那样太短视了，容易被市场的短期随机波动所迷惑。相反，他会先连续地进行 n_steps 次模拟交易 ，把这一个完整周期（比如 2048 天）的全部经历——包括每天的市场状态、他采取的行动、以及因此获得的收益或亏损——都记录在一个“经验回放缓冲区”（Rollout Buffer）里。

当这个缓冲区被装满（即完成了 n_steps 次的交互）后，他会停下来，拿出这个装满了 2048 天交易记录的“笔记本”，开始进行一次 集中的、深度的复盘和学习 。这就是模型更新（Update）的时刻。

把n_steps与total_timesteps联系起来，事情就更清晰了：

1. total_timesteps是总的学习时长。它除以n_steps，得到一个学习次数。也就是在一次训练中，会进行这么多次大的更新。
2. 在每一次大的学习更新中，模型会把一个n_steps中的数据拿出来，反复学习n_epochs次，而在每一个epoch中，又会拆分成更小的批次（batch_size）来进行梯度下降和网络权重更新（取决于内存/显存大小）。

!!! attention
    在示例中，n_steps = 2048，大约相当于8年。即每次『复盘』，智能体都经历了一个非常长的、足以包含牛市、熊市和震荡市的完整市场周期。这对于学习到一个鲁棒的、能够穿越牛熊的策略至关重要。

    不过，尽管我们设置的total_timesteps是10_000，但实际上数据只有约2880天，只够Agent进行一次大的复盘与学习更新。并且，余下的数据（约832天）因为不足以再进行一次完整的更新，这些数据没有被利用，造成了浪费。


### 回测与结果

现在，我们开始回测，并使用quantstats生成标准化的投资组合分析报表。

```python
import quantstats as qs
import matplotlib.pyplot as plt

print("📊 开始回测...")
obs, _ = test_env.reset()
done = False
total_reward = 0
step_count = 0
portfolio_values = []
dates = []

test_dates = sorted(test_data.index.get_level_values('date').unique())

while not done:
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = test_env.step(action)
    done = terminated or truncated
    total_reward += reward
    portfolio_values.append(info['portfolio_value'])

    # 记录对应的日期
    if step_count < len(test_dates):
        dates.append(test_dates[step_count])

    step_count += 1

    if step_count % 20 == 0:
        print(f"步骤 {step_count}: 投资组合价值 = ¥{info['portfolio_value']:,.2f}")

print(f"\n📈 回测完成!")
print(f"总步数: {step_count}")
print(f"总奖励: {total_reward:.4f}")
print(f"最终投资组合价值: ¥{portfolio_values[-1]:,.2f}")
print(f"总收益率: {(portfolio_values[-1] / 100000 - 1) * 100:.2f}%")

# 创建投资组合收益率序列
portfolio_df = pd.DataFrame({
    'date': dates[:len(portfolio_values)],
    'portfolio_value': portfolio_values
})
portfolio_df['date'] = pd.to_datetime(portfolio_df['date'])
portfolio_df.set_index('date', inplace=True)

# 计算每日收益率
portfolio_returns = portfolio_df['portfolio_value'].pct_change().dropna()

# 使用quantstats生成完整的投资组合分析报表
print("\n📊 生成QuantStats分析报表...")

# 设置quantstats的输出模式
qs.extend_pandas()

# 在notebook中显示关键指标
print("\n=== 投资组合绩效分析 ===")
print(f"总收益率: {qs.stats.comp(portfolio_returns):.2%}")
print(f"年化收益率: {qs.stats.cagr(portfolio_returns):.2%}")
print(f"年化波动率: {qs.stats.volatility(portfolio_returns):.2%}")
print(f"夏普比率: {qs.stats.sharpe(portfolio_returns):.3f}")
print(f"最大回撤: {qs.stats.max_drawdown(portfolio_returns):.2%}")
print(f"卡尔马比率: {qs.stats.calmar(portfolio_returns):.3f}")
print(f"胜率: {qs.stats.win_rate(portfolio_returns):.2%}")
```


总而言之，强化学习为量化交易打开了一扇通往更高维度智能的大门。它不再是让机器模仿人类，而是让机器在模拟的市场中自我进化、自我博弈，最终习得超越人类直觉的交易智慧。这条路充满挑战，但也同样充满机遇。那么，你准备好，让你的第一个交易Agent，开始它的"进化之旅"了吗？

---
1. LOXM: https://www.businessinsider.com/jpmorgan-takes-ai-use-to-the-next-level-2017-8
2. 量化交易中的强化学习：https://dl.acm.org/doi/10.1145/3582560
3. Alphastock，一个追涨杀跌模型: https://arxiv.org/abs/1908.02646


---
**注意事项：**
1. 在训练时，组合使用多少个资产，在预测时，就只能使用多少个资产。
2. 这里需要quantstats，注意它在python 3.12下完全不能运行，你需要安装quantide维护发布的quanstats-reloaded版本。
3. 数据集已按8:2比例划分为train/test两部分，PPO算法通过训练过程中的奖励曲线来监控学习进度。
